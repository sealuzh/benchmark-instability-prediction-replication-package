{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Unstable Software Benchmarks Using Static Source Code Features\n",
    "## Classification study\n",
    "\n",
    "The following Python Jupyter Notebook can be used to interactively reproduce the study we performed\n",
    "in our paper with the title *Predicting Unstable Software Benchmarks Using Static Source Code Features*.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We import the needed Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "import scikit_posthocs as sp\n",
    "from scipy import stats\n",
    "\n",
    "from study_conf_labels import *\n",
    "from study_conf_palettes import *\n",
    "from study_data_utils import *\n",
    "from study_plots_utils import *\n",
    "from study_stats_utils import *\n",
    "from utils import apply_binary_threshold, approximate_zeros, remove_negative_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "First, we configure some parameters for the script.\n",
    "\n",
    "`DATA_CSV_FILE_PATH` specifies the path for the data `CSV` file analyzed bu the notebook.\n",
    "`METRICS` is the list of metrics considered by the study.\n",
    "\n",
    "`BASELINE_MODELS` and `COMPARED_MODELS` differentiate, respectively, the list of models used as baseline and those for comparison.\n",
    "\n",
    "`ITERATIONS` and `THRESHOLDS` represent the values considered for the respective parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CSV_FILE_PATHS = {\n",
    "    5: 'resources/variabilities_5_iterations.csv',\n",
    "    10: 'resources/variabilities_10_iterations.csv',\n",
    "    20: 'resources/variabilities_20_iterations.csv',\n",
    "    30: 'resources/variabilities_30_iterations.csv',\n",
    "}\n",
    "CLASSIFICATION_RESULTS_CSV_FILE_PATH = 'resources/classification_results.csv.xz'\n",
    "PLOTS_OUTPUT_DIRECTORY_PATH = 'resources/output/plots'\n",
    "\n",
    "METRICS = ['precision', 'recall', 'fmeasure', 'auc', 'mcc']\n",
    "\n",
    "BASELINE_MODELS = [\n",
    "    'DummyClassifier(strategy=\\'most_frequent\\')', 'DummyClassifier(strategy=\\'prior\\')',\n",
    "    'DummyClassifier(strategy=\\'stratified\\')', 'DummyClassifier(strategy=\\'uniform\\')',\n",
    "]\n",
    "\n",
    "COMPARED_MODELS = [\n",
    "    'GaussianNB()', 'KNeighborsClassifier()', 'LogisticRegression()', 'MLPClassifier()', 'LinearDiscriminantAnalysis()',\n",
    "    'DecisionTreeClassifier()', 'SVC(kernel=\\'linear\\')', 'SVC(kernel=\\'rbf\\')', 'RandomForestClassifier()',\n",
    "    'AdaBoostClassifier()', 'GradientBoostingClassifier()',\n",
    "]\n",
    "\n",
    "SELECTORS = ['None', 'AutoSpearmanSelector()']\n",
    "FOCUS_SELECTORS = [\n",
    "    'None',\n",
    "]\n",
    "\n",
    "SAMPLERS = ['None', 'SMOTE()']\n",
    "FOCUS_SAMPLERS = [\n",
    "    'None',\n",
    "]\n",
    "\n",
    "PREPROCESSING_GROUPS = ['None/None', 'None/SMOTE()', 'AutoSpearmanSelector()/None', 'AutoSpearmanSelector()/SMOTE()']\n",
    "\n",
    "ITERATIONS = [\n",
    "    5,\n",
    "    10,\n",
    "    20,\n",
    "    30,\n",
    "]\n",
    "\n",
    "THRESHOLDS = [\n",
    "    1,\n",
    "    3,\n",
    "    5,\n",
    "    10,\n",
    "]\n",
    "\n",
    "CROSS_VALIDATION_FOLDS = 10\n",
    "CROSS_VALIDATION_REPETITIONS = 30\n",
    "TOTAL_CROSS_VALIDATION_FOLDS = CROSS_VALIDATION_FOLDS * CROSS_VALIDATION_REPETITIONS\n",
    "\n",
    "DEPENDENT_VARIABLES = [\n",
    "    'rciw99',\n",
    "    'rciw99mjhd',\n",
    "    'rmadhd',\n",
    "]\n",
    "FOCUS_DEPENDENT_VARIABLES = [\n",
    "    'rciw99mjhd',\n",
    "]\n",
    "\n",
    "SIGNIFICANCE_LEVEL = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study\n",
    "\n",
    "#### Distributions of stable and unstable values\n",
    "\n",
    "First of all, we read from the CSV files the data to plot the distributions considering the iterations and used thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the CSV files and concatenate them.\n",
    "iterations_df = pd.DataFrame()\n",
    "for iterations_number in ITERATIONS:\n",
    "    temp_df = pd.read_csv(DATA_CSV_FILE_PATHS[iterations_number])\n",
    "    temp_df['iterations'] = iterations_number\n",
    "    iterations_df = pd.concat([iterations_df, temp_df])\n",
    "\n",
    "# Transform the iterations column to categorical type for easy sorting.\n",
    "iterations_df['iterations'] = pd.Categorical(iterations_df['iterations'], categories=ITERATIONS)\n",
    "\n",
    "# Print the dataframe.\n",
    "display(iterations_df)\n",
    "\n",
    "# Print some statistics.\n",
    "print(f\"Number of functions: {iterations_df['function'].unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in DEPENDENT_VARIABLES:\n",
    "    # Clean the data from negative values.\n",
    "    iterations_df = remove_negative_values(iterations_df, dep_var)\n",
    "\n",
    "    # Clean the data from 0 values.\n",
    "    iterations_df = approximate_zeros(iterations_df, dep_var)\n",
    "\n",
    "# Print the dataframe.\n",
    "display(iterations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the thresholds to prepare the data for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the binarization for all the dependent variables.\n",
    "thresholds_df = pd.DataFrame()\n",
    "\n",
    "for threshold in THRESHOLDS:\n",
    "    temp_df = iterations_df[['iterations', 'project_name', 'function', *DEPENDENT_VARIABLES]]\n",
    "\n",
    "    for dep_var in DEPENDENT_VARIABLES:\n",
    "        temp_df[dep_var] = apply_binary_threshold(temp_df[dep_var], threshold)\n",
    "\n",
    "    temp_df['threshold'] = threshold\n",
    "    thresholds_df = pd.concat([thresholds_df, temp_df])\n",
    "\n",
    "# Transform the values into categorical.\n",
    "thresholds_df['iterations'] = pd.Categorical(thresholds_df['iterations'], categories=ITERATIONS)\n",
    "thresholds_df['threshold'] = pd.Categorical(thresholds_df['threshold'], categories=THRESHOLDS)\n",
    "\n",
    "# Convert the dataframe from wide to long.\n",
    "thresholds_df = pd.melt(thresholds_df, id_vars=['iterations', 'project_name', 'function', 'threshold'], value_vars=DEPENDENT_VARIABLES)\n",
    "thresholds_df['value'] = pd.Categorical(thresholds_df['value'], categories=[0, 1])\n",
    "\n",
    "display(thresholds_df)\n",
    "\n",
    "display(thresholds_df.groupby(['iterations', 'threshold', 'variable', 'value']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create and print the distribution plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    distributions_barplot = plot_distributions_bar_plot_grid(thresholds_df.query(f'variable == \"{dep_var}\"'), figure_size=(14, 1.5))\n",
    "    print(dep_var)\n",
    "    display(distributions_barplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers results\n",
    "\n",
    "We read the data from the `CSV` file, extract some elements, and computing some basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the CSV.\n",
    "df = pd.read_csv(CLASSIFICATION_RESULTS_CSV_FILE_PATH)\n",
    "\n",
    "# Select according to the given configuration.\n",
    "df = df[df['model'].isin(BASELINE_MODELS + COMPARED_MODELS)]\n",
    "df = df[df['iterations'].isin(ITERATIONS)]\n",
    "df = df[df['threshold'].isin(THRESHOLDS)]\n",
    "df = df[df['dependent_variable'].isin(DEPENDENT_VARIABLES)]\n",
    "df = df[df['selector'].isin(SELECTORS)]\n",
    "df = df[df['sampler'].isin(SAMPLERS)]\n",
    "\n",
    "# Transform some of the columns to categorical type for easy sorting.\n",
    "df['model'] = pd.Categorical(df['model'], categories=BASELINE_MODELS + COMPARED_MODELS)\n",
    "df['iterations'] = pd.Categorical(df['iterations'], categories=ITERATIONS)\n",
    "df['threshold'] = pd.Categorical(df['threshold'], categories=THRESHOLDS)\n",
    "df['dependent_variable'] = pd.Categorical(df['dependent_variable'], categories=DEPENDENT_VARIABLES)\n",
    "df['selector'] = pd.Categorical(df['selector'], categories=SELECTORS)\n",
    "df['sampler'] = pd.Categorical(df['sampler'], categories=SAMPLERS)\n",
    "\n",
    "# Create the subdataframes.\n",
    "baseline_df = df[df['model'].isin(BASELINE_MODELS)]\n",
    "baseline_df['model'] = pd.Categorical(baseline_df['model'], categories=BASELINE_MODELS)\n",
    "compared_df = df[df['model'].isin(COMPARED_MODELS)]\n",
    "compared_df['model'] = pd.Categorical(compared_df['model'], categories=COMPARED_MODELS)\n",
    "\n",
    "# Print the head of the dataframe.\n",
    "display(df)\n",
    "\n",
    "# Print some statistics.\n",
    "print(f\"Number of experiments: {df.shape[0]}\")\n",
    "print(f\"Number of folds per combination: {df['fold'].unique().shape[0]}\")\n",
    "print(f\"Models: {list(df['model'].unique())}\")\n",
    "print(f\"Benchmark iterations: {list(df['iterations'].unique())}\")\n",
    "print(f\"Stability thresholds: {list(df['threshold'].unique())}\")\n",
    "print(f\"Dependent variables: {list(df['dependent_variable'].unique())}\")\n",
    "print(f\"Selectors: {list(df['selector'].unique())}\")\n",
    "print(f\"Samplers: {list(df['sampler'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check whether there are missing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = df.groupby(['dependent_variable', 'iterations', 'threshold', 'selector', 'sampler', 'model'], as_index=False)['fold'].count()\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(experiments_df[experiments_df['fold'] != TOTAL_CROSS_VALIDATION_FOLDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality test\n",
    "\n",
    "We test if the distributions for **all** the combinations (model, iterations, and threshold) are normal.\n",
    "To do that, we apply the *Dâ€™Agostino's K^2 Test*.\n",
    "\n",
    "> *Null hypothesis*: the observations come from a normal distribution.\n",
    ">\n",
    "> `p-value >= 0.01`: accept the null hypothesis, normal\n",
    ">\n",
    "> `p-value < 0.01`: reject the null hypothesis, not normal\n",
    "\n",
    "If not all are normal, i.e., the tests accept the null hypotheses in some cases, then we use non parametric tests and plot the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    # Create a dataframe with the p-values of the normality test.\n",
    "    normal_test_df = pivot_table_grouping(\n",
    "        df.query('dependent_variable == @dep_var'),\n",
    "        index=['dependent_variable', 'model', 'iterations', 'selector', 'sampler'],\n",
    "        columns='threshold',\n",
    "        metrics=METRICS,\n",
    "        index_sort=[DEPENDENT_VARIABLES, BASELINE_MODELS + COMPARED_MODELS, ITERATIONS, SELECTORS, SAMPLERS],\n",
    "        columns_sort=[METRICS, THRESHOLDS],\n",
    "        aggfunc=lambda x: stats.normaltest(x)[1],\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    print(dep_var)\n",
    "    display(normal_test_df\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median values comparison\n",
    "\n",
    "Since there are many non-normal groups of observations, we apply non-parametric tests that compare median values.\n",
    "\n",
    "We produce a pivot table for comparing the median of each combination of model, iterations, and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe.\n",
    "median_pivot_df = pivot_table_grouping(\n",
    "    df,\n",
    "    index=['dependent_variable', 'model', 'iterations', 'selector', 'sampler'],\n",
    "    columns='threshold',\n",
    "    metrics=METRICS,\n",
    "    index_sort=[DEPENDENT_VARIABLES, BASELINE_MODELS + COMPARED_MODELS, ITERATIONS, SELECTORS, SAMPLERS],\n",
    "    columns_sort=[METRICS, THRESHOLDS],\n",
    "    aggfunc=np.median,\n",
    ")\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    # Print the dataframe showing the bars in the background.\n",
    "    print(dep_var)\n",
    "    display(median_pivot_df.query('dependent_variable == @dep_var').droplevel(0)\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show the median values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Show a background bar as indication.\n",
    "    .bar(vmin=0.0, vmax=1.0, color='#5fba7d')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the medians values for each of the target metrics.\n",
    "We variate the models on the columns.\n",
    "In the rows, we report the diffent benchmark iterations.\n",
    "For each model and iterations number, we draw a lineplot with all the stability thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            baseline_models_median_df = median_long_dataframe(\n",
    "                df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                models=BASELINE_MODELS,\n",
    "                metrics=METRICS,\n",
    "            )\n",
    "\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            baseline_models_median_lineplot = plot_metrics_comparison_lineplot_grid(\n",
    "                baseline_models_median_df,\n",
    "                models_labels=MODELS_LABELS,\n",
    "                metrics_labels=METRICS_LABELS,\n",
    "                figure_size=(7, 5),\n",
    "            )\n",
    "            display(baseline_models_median_lineplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the compared models grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            compared_models_median_df = median_long_dataframe(\n",
    "                df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                models=COMPARED_MODELS,\n",
    "                metrics=METRICS,\n",
    "            )\n",
    "\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            compared_models_median_lineplot = plot_metrics_comparison_lineplot_grid(\n",
    "                compared_models_median_df,\n",
    "                models_labels=MODELS_LABELS,\n",
    "                metrics_labels=METRICS_LABELS,\n",
    "                figure_size=(14, 5),\n",
    "            )\n",
    "            display(compared_models_median_lineplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we inspect the trends of performance from different points of view.\n",
    "In particular, we investigate considering the three dimensions involved in our study: *thresholds*, *iterations*, and *models*.\n",
    "In each of the following sections, we fix two of the dimensions and variate the other.\n",
    "\n",
    "#### Thresholds comparison\n",
    "\n",
    "For each combination of model and iterations value, we are interested in seeing if there is any significative difference when the threshold value increases.\n",
    "To do so, we test the metric values against a statistics test on median values with multiple samples of observations.\n",
    "Namely, we apply the *Kruskal-Wallis Test*, which is the non-parametric version of the *ANOVA* test.\n",
    "\n",
    "> *Null hypothesis*: the population median of all the groups are equal.\n",
    ">\n",
    "> `p-value >= 0.01`: accept the null hypothesis, equal\n",
    ">\n",
    "> `p-value < 0.01`: reject the null hypothesis, not equal\n",
    "\n",
    "Therefore, we build and display the table with the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            thresholds_kruskal_test_df = multiple_groups_test_dataframe(compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'), group_1=['model', 'iterations'], group_2='threshold', metrics=METRICS, testfunc=stats.kruskal, check_identical=True)\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            thresholds_kruskal_test_df = (\n",
    "                thresholds_kruskal_test_df.pivot_table(index=['model', 'iterations'], columns=['metric'], values=['pvalue'])\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=1)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=1)\n",
    "            )\n",
    "\n",
    "            # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(thresholds_kruskal_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS)\n",
    "            # Show the p-values with reduced decimal digits.\n",
    "            .style.format('{:.4f}')\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we confirm that the median values of the groups are statistically different, we can apply a pairwise post-hoc test to understand where this is valid. We apply the *Dunn's Test*.\n",
    "\n",
    "> *Null hypothesis*: there is no difference between the two compared groups.\n",
    ">\n",
    "> `p-value >= 0.01`: accept the null hypothesis, equal\n",
    ">\n",
    "> `p-value < 0.01`: reject the null hypothesis, not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_dunn_test_dfs = {}\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            thresholds_dunn_test_df = pairwise_multiple_groups_posthoc_test_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['model', 'iterations'],\n",
    "                group_2='threshold',\n",
    "                metrics=METRICS,\n",
    "                testfunc=sp.posthoc_dunn,\n",
    "            )\n",
    "\n",
    "            # Fix the threshold and comparison columns.\n",
    "            thresholds_dunn_test_df['threshold'] = pd.Categorical(thresholds_dunn_test_df['threshold'], categories=THRESHOLDS)\n",
    "            thresholds_dunn_test_df['comparison'] = pd.Categorical(thresholds_dunn_test_df['comparison'], categories=THRESHOLDS)\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            thresholds_dunn_test_df = (\n",
    "                thresholds_dunn_test_df.pivot_table(index=['model', 'iterations', 'threshold'], columns=['metric', 'comparison'], values=['pvalue'])\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=1)\n",
    "                # Sort the threshold.\n",
    "                .reindex(THRESHOLDS, level=2)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=1)\n",
    "                # Sort the comparison.\n",
    "                .reindex(THRESHOLDS, axis=1, level=2)\n",
    "            )\n",
    "\n",
    "            # Store the dataframe.\n",
    "            thresholds_dunn_test_dfs[(dep_var, selector, sampler)] = thresholds_dunn_test_df\n",
    "\n",
    "            # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(thresholds_dunn_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS)\n",
    "            # Show the p-values with reduced decimal digits.\n",
    "            .style.format('{:.4f}')\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: '' if x < 0 else ('background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f'))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we measure the effect size of the statistical differences by using a function that represents the *Vargha-Delaney A* test:\n",
    "\n",
    "> `A = 0.5`: the group `x` performs equal to the group `y`\n",
    ">\n",
    "> `A < 0.5`: the group `x` performs worse than the group `y`\n",
    ">\n",
    "> `A > 0.5`: the group `x` performs better than the group `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_vda_test_dfs = {}\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            thresholds_vda_test_df = pairwise_multiple_groups_vda_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['model', 'iterations'],\n",
    "                group_2='threshold',\n",
    "                metrics=METRICS,\n",
    "            )\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            thresholds_vda_test_df = (\n",
    "                thresholds_vda_test_df.pivot_table(index=['model', 'iterations', 'threshold'], columns=['metric', 'comparison'], values=['a', 'magnitude'], aggfunc='first')\n",
    "                # Use \"a\" and \"magnitude\" as an index.\n",
    "                .stack(level=0)\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=1)\n",
    "                # Sort the threshold.\n",
    "                .reindex(THRESHOLDS, level=2)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=0)\n",
    "                # Sort the comparison.\n",
    "                .reindex(THRESHOLDS, axis=1, level=1)\n",
    "            )\n",
    "\n",
    "            # Store the dataframe.\n",
    "            thresholds_vda_test_dfs[(dep_var, selector, sampler)] = thresholds_vda_test_df\n",
    "\n",
    "            # Print the dataframe showing the colored magnitude levels.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(thresholds_vda_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS)\n",
    "            # Show all the magnitude values as the shortest version.\n",
    "            .style.format(lambda x: MAGNITUDE_LABELS[x] if isinstance(x, str) else '{:.4f}'.format(x))\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterations comparison\n",
    "\n",
    "For each combination of model and threshold value, we are interested in seeing if there is any significative difference when the iterations value increases.\n",
    "Thus, we aplly the *Kruskal-Wallis Test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            iterations_kruskal_test_df = multiple_groups_test_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['model', 'threshold'],\n",
    "                group_2='iterations',\n",
    "                metrics=METRICS,\n",
    "                testfunc=stats.kruskal,\n",
    "                check_identical=True,\n",
    "            )\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            iterations_kruskal_test_df = (\n",
    "                iterations_kruskal_test_df.pivot_table(index=['model', 'threshold'], columns=['metric'], values=['pvalue'])\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "                # Sort the threshold.\n",
    "                .reindex(THRESHOLDS, level=1)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=1)\n",
    "            )\n",
    "\n",
    "            # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(iterations_kruskal_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS)\n",
    "            # Show the p-values with reduced decimal digits.\n",
    "            .style.format('{:.4f}')\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply the *Dunn's Test* as a pairwise post-hoc test to understand for which groups the median values are statistically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_dunn_test_dfs = {}\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            iterations_dunn_test_df = pairwise_multiple_groups_posthoc_test_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['model', 'threshold'],\n",
    "                group_2='iterations',\n",
    "                metrics=METRICS,\n",
    "                testfunc=sp.posthoc_dunn,\n",
    "            )\n",
    "\n",
    "            # Fix the iterations and comparison columns.\n",
    "            iterations_dunn_test_df['iterations'] = pd.Categorical(iterations_dunn_test_df['iterations'], categories=ITERATIONS)\n",
    "            iterations_dunn_test_df['comparison'] = pd.Categorical(iterations_dunn_test_df['comparison'], categories=ITERATIONS)\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            iterations_dunn_test_df = (\n",
    "                iterations_dunn_test_df.pivot_table(index=['model', 'threshold', 'iterations'], columns=['metric', 'comparison'], values=['pvalue'])\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "                # Sort the thresholds.\n",
    "                .reindex(THRESHOLDS, level=1)\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=2)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=1)\n",
    "                # Sort the comparison.\n",
    "                .reindex(ITERATIONS, axis=1, level=2)\n",
    "            )\n",
    "\n",
    "            # Store the dataframe.\n",
    "            iterations_dunn_test_dfs[(dep_var, selector, sampler)] = iterations_dunn_test_df\n",
    "\n",
    "            # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(iterations_dunn_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS)\n",
    "            # Show the p-values with reduced decimal digits.\n",
    "            .style.format('{:.4f}')\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: '' if x < 0 else ('background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f'))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we measure the effect size with the *Vargha-Delaney A* test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_vda_test_dfs = {}\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            iterations_vda_test_df = pairwise_multiple_groups_vda_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['model', 'threshold'],\n",
    "                group_2='iterations',\n",
    "                metrics=METRICS,\n",
    "            )\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            iterations_vda_test_df = (\n",
    "                iterations_vda_test_df.pivot_table(index=['model', 'threshold', 'iterations'], columns=['metric', 'comparison'], values=['a', 'magnitude'], aggfunc='first')\n",
    "                # Use \"a\" and \"magnitude\" as an index.\n",
    "                .stack(level=0)\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "                # Sort the thresholds.\n",
    "                .reindex(THRESHOLDS, level=1)\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=2)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=0)\n",
    "                # Sort the comparison.\n",
    "                .reindex(ITERATIONS, axis=1, level=1)\n",
    "            )\n",
    "\n",
    "            # Store the dataframe.\n",
    "            iterations_vda_test_dfs[(dep_var, selector, sampler)] = iterations_vda_test_df\n",
    "\n",
    "            # Print the dataframe showing the colored magnitude levels.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(iterations_vda_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS)\n",
    "            # Show all the magnitude values as the shortest version.\n",
    "            .style.format(lambda x: MAGNITUDE_LABELS[x] if isinstance(x, str) else '{:.4f}'.format(x))\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models comparison\n",
    "\n",
    "For each combination of iterations and threshold value, we are interested in seeing if there is any significative difference when the models changes.\n",
    "Thus, we aplly the *Kruskal-Wallis Test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            models_kruskal_test_df = multiple_groups_test_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['iterations', 'threshold'],\n",
    "                group_2='model',\n",
    "                metrics=METRICS,\n",
    "                testfunc=stats.kruskal,\n",
    "                check_identical=True,\n",
    "            )\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            models_kruskal_test_df = (\n",
    "                models_kruskal_test_df.pivot_table(index=['iterations', 'threshold'], columns=['metric'], values=['pvalue'])\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=0)\n",
    "                # Sort the threshold.\n",
    "                .reindex(THRESHOLDS, level=1)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=1)\n",
    "            )\n",
    "\n",
    "            # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(models_kruskal_test_df\n",
    "            # Show the p-values with reduced decimal digits.\n",
    "            .style.format('{:.4f}')\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply the *Dunn's Test* as a pairwise post-hoc test to understand for which groups the median values are statistically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            models_dunn_test_df = pairwise_multiple_groups_posthoc_test_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['iterations', 'threshold'],\n",
    "                group_2='model',\n",
    "                metrics=METRICS,\n",
    "                testfunc=sp.posthoc_dunn,\n",
    "            )\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            models_dunn_test_df = (\n",
    "                models_dunn_test_df.pivot_table(index=['iterations', 'threshold', 'model'], columns=['metric', 'comparison'], values=['pvalue'])\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=0)\n",
    "                # Sort the thresholds.\n",
    "                .reindex(THRESHOLDS, level=1)\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=2)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=1)\n",
    "                # Sort the comparison.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, axis=1, level=2)\n",
    "            )\n",
    "            \n",
    "            # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(models_dunn_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS, columns=MODELS_LABELS)\n",
    "            # Show the p-values with reduced decimal digits.\n",
    "            .style.format('{:.4f}')\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: '' if x < 0 else ('background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f'))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we measure the effect size with the *Vargha-Delaney A* test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            # Create the dataframe.\n",
    "            models_vda_test_df = pairwise_multiple_groups_vda_dataframe(\n",
    "                compared_df.query('dependent_variable == @dep_var and selector == @selector and sampler == @sampler'),\n",
    "                group_1=['iterations', 'threshold'],\n",
    "                group_2='model',\n",
    "                metrics=METRICS,\n",
    "            )\n",
    "\n",
    "            # Pivot the dataframe for better visualization.\n",
    "            models_vda_test_df = (\n",
    "                models_vda_test_df.pivot_table(index=['iterations', 'threshold', 'model'], columns=['metric', 'comparison'], values=['a', 'magnitude'], aggfunc='first')\n",
    "                # Use \"a\" and \"magnitude\" as an index.\n",
    "                .stack(level=0)\n",
    "                # Sort the iterations.\n",
    "                .reindex(ITERATIONS, level=0)\n",
    "                # Sort the thresholds.\n",
    "                .reindex(THRESHOLDS, level=1)\n",
    "                # Sort the models.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, level=2)\n",
    "                # Sort the metrics.\n",
    "                .reindex(METRICS, axis=1, level=0)\n",
    "                # Sort the comparison.\n",
    "                .reindex(BASELINE_MODELS + COMPARED_MODELS, axis=1, level=1)\n",
    "            )\n",
    "            \n",
    "            # Print the dataframe showing the colored magnitude levels.\n",
    "            print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}')\n",
    "            display(models_vda_test_df\n",
    "            # Rename all the model names into the shortest version.\n",
    "            .rename(index=MODELS_LABELS, columns=MODELS_LABELS)\n",
    "            # Show all the magnitude values as the shortest version.\n",
    "            .style.format(lambda x: MAGNITUDE_LABELS[x] if isinstance(x, str) else '{:.4f}'.format(x))\n",
    "            # Apply the color filtering.\n",
    "            .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best models comparison.\n",
    "\n",
    "Rank the top 10 models by MCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    # Create the dataframe.\n",
    "    best_mcc_df = median_pivot_df[['mcc', 'auc', 'fmeasure', 'precision', 'recall']].query('dependent_variable == @dep_var').droplevel(0).stack().sort_values(by=['mcc', 'auc', 'fmeasure', 'precision', 'recall'], ascending=False).reindex(columns=['mcc', 'auc', 'fmeasure', 'precision', 'recall']).reset_index()\n",
    "\n",
    "    # Print the dataframe showing the bars in the background.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(best_mcc_df.head(10)\n",
    "    # Show the median values with reduced decimal digits.\n",
    "    .style.format({'model': lambda x: MODELS_LABELS[x], **{x: '{:.4f}' for x in METRICS}})\n",
    "    # Show a background bar as indication.\n",
    "    .bar(vmin=0.0, vmax=1.0, color='#5fba7d')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_median_pivot_df = median_pivot_df.query(f'dependent_variable == \"{dep_var}\" and model == \"RandomForestClassifier()\"')\n",
    "\n",
    "# Print the dataframe showing the bars in the background.\n",
    "display(rf_median_pivot_df\n",
    "# Rename all the model names into the shortest version.\n",
    ".rename(index=MODELS_LABELS)\n",
    "# Show the median values with reduced decimal digits.\n",
    ".style.format('{:.4f}')\n",
    "# Show a background bar as indication.\n",
    ".bar(vmin=0.0, vmax=1.0, color='#5fba7d')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Threshold comparison\n",
    "\n",
    "Verify the Dunn's pairs for threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            for metric in ['mcc', 'auc']:\n",
    "                pairs_thresholds_dunn_test_df = thresholds_dunn_test_dfs[(dep_var, selector, sampler)]['pvalue', metric].stack().to_frame('pvalue').reset_index()\n",
    "                pairs_thresholds_dunn_test_df['model'] = pd.Categorical(pairs_thresholds_dunn_test_df['model'], categories=BASELINE_MODELS + COMPARED_MODELS)\n",
    "\n",
    "                print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}, metric={metric}')\n",
    "                \n",
    "                # Extract the values for (1, 3), (3, 5), (5, 10)\n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_thresholds_dunn_test_df, pd.DataFrame(columns=['threshold', 'comparison'], data=[(1, 3), (3, 5), (5, 10)]), on=['threshold', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'iterations', 'threshold', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f', subset='pvalue')\n",
    "                )\n",
    "\n",
    "                # Extract the values for (1, 10) \n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_thresholds_dunn_test_df, pd.DataFrame(columns=['threshold', 'comparison'], data=[(1, 10)]), on=['threshold', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'iterations', 'threshold', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f', subset='pvalue')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Vargha's pairs for threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            for metric in ['mcc', 'auc']:\n",
    "                pairs_thresholds_vda_test_df = thresholds_vda_test_dfs[(dep_var, selector, sampler)][metric].stack().to_frame('value').reset_index().rename(columns={'level_3': 'variable'})[\n",
    "                    ['model', 'iterations', 'threshold', 'comparison', 'variable', 'value']]\n",
    "                pairs_thresholds_vda_test_df['model'] = pd.Categorical(pairs_thresholds_vda_test_df['model'], categories=BASELINE_MODELS + COMPARED_MODELS)\n",
    "\n",
    "                # Sets a column for growing values.\n",
    "                pairs_thresholds_vda_test_df['growing'] = pairs_thresholds_vda_test_df.loc[pairs_thresholds_vda_test_df['variable'] == 'a']['value'] < 0.5\n",
    "\n",
    "                print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}, metric={metric}')\n",
    "\n",
    "                # Extract the values for (1, 3), (3, 5), (5, 10)\n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_thresholds_vda_test_df, pd.DataFrame(columns=['threshold', 'comparison'], data=[(1, 3), (3, 5), (5, 10)]), on=['threshold', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'iterations', 'threshold', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '', subset='value')\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x is True else ('background-color: #d65f5f' if x is False else ''), subset='growing')\n",
    "                )\n",
    "\n",
    "                # Extract the values for (1, 10) \n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_thresholds_vda_test_df, pd.DataFrame(columns=['threshold', 'comparison'], data=[(1, 10)]), on=['threshold', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'iterations', 'threshold', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '', subset='value')\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x is True else ('background-color: #d65f5f' if x is False else ''), subset='growing')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iterations comparison\n",
    "\n",
    "Verify the Dunn's pairs for iterations values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            for metric in ['mcc', 'auc']:\n",
    "                pairs_iterations_dunn_test_df = iterations_dunn_test_dfs[(dep_var, selector, sampler)]['pvalue', metric].stack().to_frame('pvalue').reset_index()\n",
    "                pairs_iterations_dunn_test_df['model'] = pd.Categorical(pairs_iterations_dunn_test_df['model'], categories=BASELINE_MODELS + COMPARED_MODELS)\n",
    "\n",
    "                print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}, metric={metric}')\n",
    "\n",
    "                # Extract the values for (5, 10), (10, 20), (20, 30)\n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_iterations_dunn_test_df, pd.DataFrame(columns=['iterations', 'comparison'], data=[(5, 10), (10, 20), (20, 30)]), on=['iterations', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'threshold', 'iterations', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f', subset='pvalue')\n",
    "                )\n",
    "\n",
    "                # Extract the values for (5, 30) \n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_iterations_dunn_test_df, pd.DataFrame(columns=['iterations', 'comparison'], data=[(5, 30)]), on=['iterations', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'threshold', 'iterations', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f', subset='pvalue')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Vargha's pairs for iterations values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    for selector in FOCUS_SELECTORS:\n",
    "        for sampler in FOCUS_SAMPLERS:\n",
    "            for metric in ['mcc', 'auc']:\n",
    "                pairs_iterations_vda_test_df = iterations_vda_test_dfs[(dep_var, selector, sampler)][metric].stack().to_frame('value').reset_index().rename(columns={'level_3': 'variable'})[\n",
    "                    ['model', 'threshold', 'iterations', 'comparison', 'variable', 'value']]\n",
    "                pairs_iterations_vda_test_df['model'] = pd.Categorical(pairs_iterations_vda_test_df['model'], categories=BASELINE_MODELS + COMPARED_MODELS)\n",
    "\n",
    "                # Sets a column for growing values.\n",
    "                pairs_iterations_vda_test_df['growing'] = pairs_iterations_vda_test_df.loc[pairs_iterations_vda_test_df['variable'] == 'a']['value'] < 0.5\n",
    "\n",
    "                print(f'dep_var={dep_var}, selector={selector}, sampler={sampler}, metric={metric}')\n",
    "\n",
    "                # Extract the values for (5, 10), (10, 20), (20, 30)\n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_iterations_vda_test_df, pd.DataFrame(columns=['iterations', 'comparison'], data=[(5, 10), (10, 20), (20, 30)]), on=['iterations', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'threshold', 'iterations', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '', subset='value')\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x is True else ('background-color: #d65f5f' if x is False else ''), subset='growing')\n",
    "                )\n",
    "\n",
    "                # Extract the values for (5, 30) \n",
    "                # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "                display(pd.merge(pairs_iterations_vda_test_df, pd.DataFrame(columns=['iterations', 'comparison'], data=[(5, 30)]), on=['iterations', 'comparison'])\n",
    "                # Sort the values of the columns.\n",
    "                .sort_values(['model', 'threshold', 'iterations', 'comparison'])\n",
    "                # Format cells.\n",
    "                .style.format({'model': lambda x: MODELS_LABELS[x], 'pvalue': '{:.2f}'})\n",
    "                # Apply the color filtering.\n",
    "                .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '', subset='value')\n",
    "                .applymap(lambda x: 'background-color: #5fba7d' if x is True else ('background-color: #d65f5f' if x is False else ''), subset='growing')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing comparison\n",
    "\n",
    "We now analyze the impact of the use of some preprocessing steps on the classification performance.\n",
    "We first plot the overall preprocessing comparison plot.\n",
    "Then, we plot the comparison plot by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_dfs = {}\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    # Transform the dataframe from wide to long.\n",
    "    preprocessing_df = pd.melt(df.query('dependent_variable == @dep_var'), id_vars=['dependent_variable', 'model', 'iterations', 'threshold', 'selector', 'sampler', 'fold'])\n",
    "\n",
    "    # Create a column for the selector/sampler combinations.\n",
    "    preprocessing_df['group'] = preprocessing_df['selector'].astype(str) + '/' + preprocessing_df['sampler'].astype(str)\n",
    "\n",
    "    # Filter the data.\n",
    "    preprocessing_df = preprocessing_df[preprocessing_df['model'].isin(COMPARED_MODELS)]\n",
    "    preprocessing_df = preprocessing_df[preprocessing_df['variable'].isin(METRICS)]\n",
    "    preprocessing_df = preprocessing_df[preprocessing_df['group'].isin(PREPROCESSING_GROUPS)]\n",
    "\n",
    "    # Transform the values into categorical.\n",
    "    preprocessing_df['variable'] = pd.Categorical(preprocessing_df['variable'], categories=METRICS)\n",
    "    preprocessing_df['group'] = pd.Categorical(preprocessing_df['group'], categories=PREPROCESSING_GROUPS)\n",
    "\n",
    "    print(f'dep_var={dep_var}')\n",
    "\n",
    "    preprocessing_boxplot_overall = plot_preprocessing_boxplot_overall(\n",
    "        preprocessing_df,\n",
    "        metrics_labels=METRICS_LABELS,\n",
    "        groups_labels=PREPROCESSING_GROUPS_LABELS,\n",
    "        figure_size=(6, 10),\n",
    "    )\n",
    "    display(preprocessing_boxplot_overall)\n",
    "\n",
    "    preprocessing_boxplot_bymodel = plot_preprocessing_boxplot_bymodel(\n",
    "        preprocessing_df,\n",
    "        models_labels=MODELS_LABELS,\n",
    "        metrics_labels=METRICS_LABELS,\n",
    "        groups_labels=PREPROCESSING_GROUPS_LABELS,\n",
    "        figure_size=(6, 20),\n",
    "    )\n",
    "    display(preprocessing_boxplot_bymodel)\n",
    "\n",
    "    # Store the dataframe.\n",
    "    preprocessing_df = preprocessing_df.pivot(index=['dependent_variable', 'iterations', 'threshold', 'model', 'group', 'fold'], columns='variable', values='value')\n",
    "    preprocessing_dfs[dep_var] = preprocessing_df\n",
    "    display(preprocessing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same, but showing the pairwise differences values instead of absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences_preprocessing_dfs = {}\n",
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    preprocessing_df = df.query('dependent_variable == @dep_var')\n",
    "\n",
    "    # Create a column for the selector/sampler combinations.\n",
    "    preprocessing_df['group'] = preprocessing_df['selector'].astype(str) + '/' + preprocessing_df['sampler'].astype(str)\n",
    "\n",
    "    # Filter the data.\n",
    "    preprocessing_df = preprocessing_df[preprocessing_df['model'].isin(COMPARED_MODELS)]\n",
    "    preprocessing_df = preprocessing_df[preprocessing_df['group'].isin(PREPROCESSING_GROUPS)]\n",
    "\n",
    "    # Separate the baseline from the comparison rows.\n",
    "    baseline_preprocessing_df = preprocessing_df.query('group == \"None/None\"')\n",
    "    comparison_preprocessing_df = preprocessing_df.query('group != \"None/None\"')\n",
    "\n",
    "    # Merge on the combinations, except for selector and sampler.\n",
    "    differences_preprocessing_df = comparison_preprocessing_df.merge(baseline_preprocessing_df, on=['dependent_variable', 'iterations', 'threshold', 'model', 'fold'])\n",
    "\n",
    "    # Adjust the dataframe and compute the differences.\n",
    "    differences_preprocessing_df['group'] = differences_preprocessing_df['group_x']\n",
    "    for metric in METRICS:\n",
    "        differences_preprocessing_df[metric] = differences_preprocessing_df[f'{metric}_y'] - differences_preprocessing_df[f'{metric}_x']\n",
    "\n",
    "    # Transform the dataframe from wide to long.\n",
    "    differences_preprocessing_df = pd.melt(differences_preprocessing_df, id_vars=['dependent_variable', 'model', 'iterations', 'threshold', 'fold', 'group'], value_vars=METRICS)\n",
    "\n",
    "    # Filter the data.\n",
    "    differences_preprocessing_df = differences_preprocessing_df[differences_preprocessing_df['variable'].isin(METRICS)]\n",
    "\n",
    "    # Transform the values into categorical.\n",
    "    differences_preprocessing_df['variable'] = pd.Categorical(differences_preprocessing_df['variable'], categories=METRICS)\n",
    "    differences_preprocessing_df['group'] = pd.Categorical(differences_preprocessing_df['group'], categories=[x for x in PREPROCESSING_GROUPS if x != 'None/None'])\n",
    "\n",
    "    print(f'dep_var={dep_var}')\n",
    "\n",
    "    differences_preprocessing_boxplot_overall = plot_preprocessing_boxplot_overall(\n",
    "        differences_preprocessing_df,\n",
    "        metrics_labels=METRICS_LABELS,\n",
    "        groups_labels=PREPROCESSING_GROUPS_LABELS,\n",
    "        figure_size=(6, 10),\n",
    "    )\n",
    "    display(differences_preprocessing_boxplot_overall)\n",
    "\n",
    "    differences_preprocessing_boxplot_bymodel = plot_preprocessing_boxplot_bymodel(\n",
    "        differences_preprocessing_df,\n",
    "        models_labels=MODELS_LABELS,\n",
    "        metrics_labels=METRICS_LABELS,\n",
    "        groups_labels=PREPROCESSING_GROUPS_LABELS,\n",
    "        figure_size=(6, 20),\n",
    "    )\n",
    "    display(differences_preprocessing_boxplot_bymodel)\n",
    "\n",
    "    # Store the dataframe.\n",
    "    differences_preprocessing_df = differences_preprocessing_df.pivot(index=['dependent_variable', 'iterations', 'threshold', 'model', 'group', 'fold'], columns='variable', values='value')\n",
    "    differences_preprocessing_dfs[dep_var] = differences_preprocessing_df\n",
    "    display(differences_preprocessing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the difference values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    differences_preprocessing_df = differences_preprocessing_dfs[dep_var]\n",
    "\n",
    "    overall_median_df = pivot_table_grouping(\n",
    "        differences_preprocessing_df,\n",
    "        index='dependent_variable',\n",
    "        columns='group',\n",
    "        metrics=METRICS,\n",
    "        index_sort=None,\n",
    "        columns_sort=[METRICS, PREPROCESSING_GROUPS],\n",
    "        aggfunc=np.median,\n",
    "    )\n",
    "    display(overall_median_df)\n",
    "\n",
    "    bymodel_median_df = pivot_table_grouping(\n",
    "        differences_preprocessing_df,\n",
    "        index=['dependent_variable', 'model'],\n",
    "        columns='group',\n",
    "        metrics=METRICS,\n",
    "        index_sort=[DEPENDENT_VARIABLES, BASELINE_MODELS + COMPARED_MODELS],\n",
    "        columns_sort=[METRICS, PREPROCESSING_GROUPS],\n",
    "        aggfunc=np.median,\n",
    "    )\n",
    "    display(bymodel_median_df)\n",
    "\n",
    "    display(bymodel_median_df.query('model == \"RandomForestClassifier()\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify the normality of the distributions by using the *Dâ€™Agostino's K^2 Test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    preprocessing_df = preprocessing_dfs[dep_var]\n",
    "\n",
    "    # Create a dataframe with the p-values of the normality test.\n",
    "    overall_normal_test_df = pivot_table_grouping(\n",
    "        preprocessing_df,\n",
    "        index='dependent_variable',\n",
    "        columns='group',\n",
    "        metrics=METRICS,\n",
    "        index_sort=None,\n",
    "        columns_sort=[METRICS, PREPROCESSING_GROUPS],\n",
    "        aggfunc=lambda x: stats.normaltest(x)[1],\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(overall_normal_test_df\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "    )\n",
    "\n",
    "    # Create a dataframe with the p-values of the normality test.\n",
    "    bymodel_normal_test_df = pivot_table_grouping(\n",
    "        preprocessing_df,\n",
    "        index=['dependent_variable', 'model'],\n",
    "        columns='group',\n",
    "        metrics=METRICS,\n",
    "        index_sort=[DEPENDENT_VARIABLES, BASELINE_MODELS + COMPARED_MODELS],\n",
    "        columns_sort=[METRICS, PREPROCESSING_GROUPS],\n",
    "        aggfunc=lambda x: stats.normaltest(x)[1],\n",
    "    )\n",
    "\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    display(bymodel_normal_test_df\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test whether there are any statiscally significant differences between the groups by using the *Kruskall-Wallis Test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    preprocessing_df = preprocessing_dfs[dep_var]\n",
    "\n",
    "    # Create the dataframe.\n",
    "    overall_kruskal_test_df = multiple_groups_test_dataframe(preprocessing_df, group_1='dependent_variable', group_2='group', metrics=METRICS, testfunc=stats.kruskal, check_identical=True)\n",
    "\n",
    "    # Pivot the dataframe for better visualization.\n",
    "    overall_kruskal_test_df = (\n",
    "        overall_kruskal_test_df.pivot_table(index=['dependent_variable'], columns=['metric'], values=['pvalue'])\n",
    "        # Sort the dependent variables.\n",
    "        .reindex(FOCUS_DEPENDENT_VARIABLES)\n",
    "        # Sort the metrics.\n",
    "        .reindex(METRICS, level=1, axis=1)\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(overall_kruskal_test_df\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "    )\n",
    "\n",
    "    # Create the dataframe.\n",
    "    bymodel_kruskal_test_df = multiple_groups_test_dataframe(preprocessing_df, group_1=['dependent_variable', 'model'], group_2='group', metrics=METRICS, testfunc=stats.kruskal, check_identical=True)\n",
    "\n",
    "    # Pivot the dataframe for better visualization.\n",
    "    bymodel_kruskal_test_df = (\n",
    "        bymodel_kruskal_test_df.pivot_table(index=['dependent_variable', 'model'], columns=['metric'], values=['pvalue'])\n",
    "        # Sort the dependent variables.\n",
    "        .reindex(FOCUS_DEPENDENT_VARIABLES, level=0)\n",
    "        # Sort the models.\n",
    "        .reindex(BASELINE_MODELS + COMPARED_MODELS, level=1)\n",
    "        # Sort the metrics.\n",
    "        .reindex(METRICS, level=1, axis=1)\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(bymodel_kruskal_test_df\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: 'background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the *Dunn's Test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    preprocessing_df = preprocessing_dfs[dep_var]\n",
    "\n",
    "    # Create the dataframe.\n",
    "    overall_dunn_test_df = pairwise_multiple_groups_posthoc_test_dataframe(\n",
    "        pd.DataFrame(preprocessing_df.to_records()),\n",
    "        group_1='dependent_variable',\n",
    "        group_2='group',\n",
    "        metrics=METRICS,\n",
    "        testfunc=sp.posthoc_dunn,\n",
    "    )\n",
    "\n",
    "    # Fix the columns.\n",
    "    overall_dunn_test_df['group'] = pd.Categorical(overall_dunn_test_df['group'], categories=PREPROCESSING_GROUPS)\n",
    "    overall_dunn_test_df['comparison'] = pd.Categorical(overall_dunn_test_df['comparison'], categories=PREPROCESSING_GROUPS)\n",
    "\n",
    "    # Pivot the dataframe for better visualization.\n",
    "    overall_dunn_test_df = (\n",
    "        overall_dunn_test_df.pivot_table(index=['group'], columns=['metric', 'comparison'], values=['pvalue'])\n",
    "        # Sort the groups.\n",
    "        .reindex(PREPROCESSING_GROUPS)\n",
    "        # Sort the metrics.\n",
    "        .reindex(METRICS, axis=1, level=1)\n",
    "        # Sort the comparison.\n",
    "        .reindex(PREPROCESSING_GROUPS, axis=1, level=2)\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(overall_dunn_test_df\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: '' if x < 0 else ('background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f'))\n",
    "    )\n",
    "\n",
    "    # Create the dataframe.\n",
    "    bymodel_dunn_test_df = pairwise_multiple_groups_posthoc_test_dataframe(\n",
    "        pd.DataFrame(preprocessing_df.to_records()),\n",
    "        group_1='model',\n",
    "        group_2='group',\n",
    "        metrics=METRICS,\n",
    "        testfunc=sp.posthoc_dunn,\n",
    "    )\n",
    "\n",
    "    # Fix the columns.\n",
    "    bymodel_dunn_test_df['group'] = pd.Categorical(bymodel_dunn_test_df['group'], categories=PREPROCESSING_GROUPS)\n",
    "    bymodel_dunn_test_df['comparison'] = pd.Categorical(bymodel_dunn_test_df['comparison'], categories=PREPROCESSING_GROUPS)\n",
    "\n",
    "    # Pivot the dataframe for better visualization.\n",
    "    bymodel_dunn_test_df = (\n",
    "        bymodel_dunn_test_df.pivot_table(index=['model', 'group'], columns=['metric', 'comparison'], values=['pvalue'])\n",
    "        # Sort the models.\n",
    "        .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "        # Sort the groups.\n",
    "        .reindex(PREPROCESSING_GROUPS, level=1)\n",
    "        # Sort the metrics.\n",
    "        .reindex(METRICS, axis=1, level=1)\n",
    "        # Sort the comparison.\n",
    "        .reindex(PREPROCESSING_GROUPS, axis=1, level=2)\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the acceptance of the alternative hypothesis as green, and reject as red.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(bymodel_dunn_test_df\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show the p-values with reduced decimal digits.\n",
    "    .style.format('{:.4f}')\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: '' if x < 0 else ('background-color: #5fba7d' if x < SIGNIFICANCE_LEVEL else 'background-color: #d65f5f'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *Vargha-Delaney Test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in FOCUS_DEPENDENT_VARIABLES:\n",
    "    preprocessing_df = preprocessing_dfs[dep_var]\n",
    "\n",
    "    # Create the dataframe.\n",
    "    overall_vda_test_df = pairwise_multiple_groups_vda_dataframe(\n",
    "        pd.DataFrame(preprocessing_df.to_records()),\n",
    "        group_1='dependent_variable',\n",
    "        group_2='group',\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "\n",
    "    # Pivot the dataframe for better visualization.\n",
    "    overall_vda_test_df = (\n",
    "        overall_vda_test_df.pivot_table(index=['group'], columns=['metric', 'comparison'], values=['a', 'magnitude'], aggfunc='first')\n",
    "        # Use \"a\" and \"magnitude\" as an index.\n",
    "        .stack(level=0)\n",
    "        # Sort the groups.\n",
    "        .reindex(PREPROCESSING_GROUPS, level=0)\n",
    "        # Sort the metrics.\n",
    "        .reindex(METRICS, axis=1, level=0)\n",
    "        # Sort the comparison.\n",
    "        .reindex(PREPROCESSING_GROUPS, axis=1, level=1)\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the colored magnitude levels.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(overall_vda_test_df\n",
    "    # Show all the magnitude values as the shortest version.\n",
    "    .style.format(lambda x: MAGNITUDE_LABELS[x] if isinstance(x, str) else '{:.4f}'.format(x))\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '')\n",
    "    )\n",
    "\n",
    "    # Create the dataframe.\n",
    "    bymodel_vda_test_df = pairwise_multiple_groups_vda_dataframe(\n",
    "        pd.DataFrame(preprocessing_df.to_records()),\n",
    "        group_1='model',\n",
    "        group_2='group',\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "\n",
    "    # Pivot the dataframe for better visualization.\n",
    "    bymodel_vda_test_df = (\n",
    "        bymodel_vda_test_df.pivot_table(index=['model', 'group'], columns=['metric', 'comparison'], values=['a', 'magnitude'], aggfunc='first')\n",
    "        # Use \"a\" and \"magnitude\" as an index.\n",
    "        .stack(level=0)\n",
    "        # Sort the models.\n",
    "        .reindex(BASELINE_MODELS + COMPARED_MODELS, level=0)\n",
    "        # Sort the groups.\n",
    "        .reindex(PREPROCESSING_GROUPS, level=1)\n",
    "        # Sort the metrics.\n",
    "        .reindex(METRICS, axis=1, level=0)\n",
    "        # Sort the comparison.\n",
    "        .reindex(PREPROCESSING_GROUPS, axis=1, level=1)\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the colored magnitude levels.\n",
    "    display(bymodel_vda_test_df\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show all the magnitude values as the shortest version.\n",
    "    .style.format(lambda x: MAGNITUDE_LABELS[x] if isinstance(x, str) else '{:.4f}'.format(x))\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '')\n",
    "    )\n",
    "\n",
    "    # Print the dataframe showing the colored magnitude levels.\n",
    "    display(bymodel_vda_test_df.query('model == \"RandomForestClassifier()\"')\n",
    "    # Rename all the model names into the shortest version.\n",
    "    .rename(index=MODELS_LABELS)\n",
    "    # Show all the magnitude values as the shortest version.\n",
    "    .style.format(lambda x: MAGNITUDE_LABELS[x] if isinstance(x, str) else '{:.4f}'.format(x))\n",
    "    # Apply the color filtering.\n",
    "    .applymap(lambda x: f'background-color: {MAGNITUDE_PALETTE[x]}' if x in MAGNITUDE_PALETTE else '')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent variables comparison\n",
    "\n",
    "We now analyze the differences in predictive performance with the other dependent variables.\n",
    "For each of the variables, we print the top 10 models with the highest MCC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dep_var in DEPENDENT_VARIABLES:\n",
    "    # Create the dataframe.\n",
    "    best_mcc_df = median_pivot_df[['mcc', 'auc', 'fmeasure', 'precision', 'recall']].query('dependent_variable == @dep_var').droplevel(0).stack().sort_values(by=['mcc', 'auc', 'fmeasure', 'precision', 'recall'], ascending=False).reindex(columns=['mcc', 'auc', 'fmeasure', 'precision', 'recall']).reset_index()\n",
    "\n",
    "    # Print the dataframe showing the bars in the background.\n",
    "    print(f'dep_var={dep_var}')\n",
    "    display(best_mcc_df.head(10)\n",
    "    # Show the median values with reduced decimal digits.\n",
    "    .style.format({'model': lambda x: MODELS_LABELS[x], **{x: '{:.4f}' for x in METRICS}})\n",
    "    # Show a background bar as indication.\n",
    "    .bar(vmin=0.0, vmax=1.0, color='#5fba7d')\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
